{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.12.3' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # !pip install pandas openpyx\n",
    "# # !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install seaborn\n",
    "# !pip install plotly\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from datetime import datetime, timedelta\n",
    "import openpyxl\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/viren/OneDrive/Desktop/IIT-MADARAS(GUVI)/Industrial Copper Modeling/Copper_Set.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the number of unique value in each features\n",
    "for i in list(df.columns):\n",
    "    print(f\"{i}: {df[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert quantity tons to numeric\n",
    "df['quantity tons'] = pd.to_numeric(df['quantity tons'], errors='coerce')\n",
    "\n",
    "# Ensure dates are treated as strings to avoid float issues\n",
    "df['item_date_1'] = df['item_date'].astype(str).str.replace('\\.0$', '', regex=True)\n",
    "\n",
    "df['delivery date_1'] = df['delivery date'].astype(str).str.replace('\\.0$', '', regex=True)\n",
    "\n",
    "# Convert to datetime with correct format\n",
    "df['item_date_1'] = pd.to_datetime(df['item_date'], format='%Y%m%d', errors='coerce')\n",
    "df['delivery date_1'] = pd.to_datetime(df['delivery date'], format='%Y%m%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify NaN values in 'Material_reference' after replacement in % \n",
    "print(np.round(df['material_ref'].isnull().mean()*100, 5),\"% of missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id', 'material_ref'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity tons and selling price values are not below 0. so we convert to null below 0 values.\n",
    "\n",
    "df['quantity tons'] = df['quantity tons'].apply(lambda x: np.nan if x<=0 else x)\n",
    "df['selling_price'] = df['selling_price'].apply(lambda x: np.nan if x<=0 else x)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null values in this dataFrame:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling null values using median and mode\n",
    "# identifying the object and numerical columns\n",
    "object_columns = ['item_date','item_date_1','delivery date','delivery date_1','status']\n",
    "numerical_columns = ['quantity tons','customer','country','application','thickness','selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median - middle value in dataset (asc/desc), mode - value that appears most freqently in dataset\n",
    "# Fill missing values in object columns with mode\n",
    "for col in object_columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# fill missing values in numerical columns with median\n",
    "for col in numerical_columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicking for the unique values of all the columns:\n",
    "\n",
    "for i in df.columns:\n",
    "    print(i,\":\",df[i].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['country','status','item type','application']:\n",
    "    print(col,df[col].unique())\n",
    "    print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['status','item type']:\n",
    "    print(df[col].value_counts())\n",
    "    print('--'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'] = df['status'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertca tegorical columns to numerical using OrdinalEncoder and using map method.\n",
    "df['status'] = df['status'].map({'Lost': 0,'Won': 1,'Draft': 2,'To be approved': 3,\n",
    "                                  'Not lost for AM': 4,'Wonderful': 5,'Revised': 6,'Offered': 7,'Offerable': 8})\n",
    "\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item type'] = OrdinalEncoder().fit_transform(df[['item type']])\n",
    "\n",
    "df['item type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness Handling - Feature Scaling(Log Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers - box plot & skewes data - hist plot and violin plot\n",
    "\n",
    "def plot(df, column):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    sns.boxplot(data=df, x=column)\n",
    "    plt.title(f'Box Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=50)\n",
    "    plt.title(f'Distribution Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    sns.violinplot(data=df, x=column)\n",
    "    plt.title(f'Violin plot for {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['quantity tons','customer','country','item type','application','thickness','width', 'selling_price']:\n",
    "    plot(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantity tons, thickness and selling price data are skewed, so we will apply log transformation to these columns.\n",
    "df1 = df.copy()\n",
    "df1['quantity tons_log'] = np.log(df1['quantity tons'])\n",
    "df1['thickness_log'] = np.log(df1['thickness'])\n",
    "df1['selling_price_log'] = np.log(df1['selling_price'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after log transformation the data are normally distributed and reduced the skewness. [hist plot and violin plot]\n",
    "for i in ['quantity tons_log','thickness_log','width', 'selling_price_log']:\n",
    "    plot(df1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outleries Handling - InterquartileR ange (IQR) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using IQR and clip() methods to handle outliers and add a new column of DataFrame\n",
    "\n",
    "def outlier(df, column):\n",
    "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
    "    lower_threshold = df[column].quantile(0.25) - 1.5 * iqr\n",
    "    upper_threshold = df[column].quantile(0.75) + 1.5 * iqr\n",
    "    df[column] = df[column].clip(lower_threshold, upper_threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Ex: lower threshold = 5 and upper threshold = 20)\n",
    "# aboveu  pper threshold values (>20) are converted tou peer threshold value (20) in features\n",
    "# below lower threshold values (<5) are converted to lower threshold value (5) in features\n",
    "\n",
    "outlier(df2,'quantity tons_log')\n",
    "outlier(df2,'thickness_log')\n",
    "outlier(df2,'selling_price_log')\n",
    "outlier(df2,'width')\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the outliers to within range using IQR and clip() methods - box plot\n",
    "\n",
    "for i in ['quantity tons_log','thickness_log','selling_price_log','width']:\n",
    "    plot(df2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After add the new columns of 'quantity tons_log', thickness_log','selling_price_log', and 'width', drop the existing columns of 'quantity tons', 'thickness', and 'selling_price' columns.\n",
    "df3 = df2.drop(columns=['quantity tons', 'thickness', 'selling_price'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to veryfy any columns are highly correlated using Heatmap. if any columns correlation value >= 0.7 (absolute value), drop the columns\n",
    "\n",
    "col = ['quantity tons_log','customer','country','status','application','width','product_ref','thickness_log','selling_price_log']\n",
    "df_heatmap = df3[col].corr()\n",
    "sns.heatmap(df_heatmap, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest value is (0.4 or -0.42) only, So there is no column are highly correelated and no need to drop any columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'delivery date' is previous date of 'item date'. so this is impossible. delivery date is always greater.\n",
    "# so finding the difference between 'delivery date' and 'item date' and adding a new column of 'delivery_date_diff' in df4 DataFrame.\n",
    "df4['delivery_date_diff'] = (df4['delivery date_1'] - df4['item_date_1']).dt.days\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dat type using pandas\n",
    "df4['item_date_1'] = pd.to_datetime(df4['item_date_1'])\n",
    "\n",
    "# split the day, month, and year from 'item_date_1' column and add dataframe (this dataframe is used for modeling)\n",
    "df4['item_date_day'] = df4['item_date_1'].dt.day\n",
    "df4['item_date_month'] = df4['item_date_1'].dt.month\n",
    "df4['item_date_year'] = df4['item_date_1'].dt.year  \n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the non-negative value of 'Data_difference' column in separate dataFrame\n",
    "df_f1 = df4[df4['delivery_date_diff'] >= 0]\n",
    "\n",
    "# after split, the index values are unordered. so reset the index  to ascending order from 0\n",
    "df_f1 = df_f1.reset_index(drop=True)\n",
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the negative value of 'delivery_date_diff' column in another dataframe\n",
    "df_f2 = df4[df4['delivery_date_diff'] < 0]\n",
    "df_f2 = df_f2.reset_index(drop=True)\n",
    "df_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 16108 values 'delivery date' are lesser than 'item date'.\n",
    "# First we need to train the ML model using correct 'Delivery date' data (df_f1) and predict the 'dadelivery_date_diff' for 'df_f2' DataFrame. using ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best algorithm to prediction based on R2, meanabsolute error,mean squared error values\n",
    "\n",
    "def machine_learning_delivery_date(df, algorithm):\n",
    "\n",
    "    x = df.drop(columns=['item_date_1', 'delivery date_1', 'delivery_date_diff'], axis=1)\n",
    "    y = df['delivery_date_diff']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    model = algorithm().fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    metrics = {'Algorithm': algorithm.__name__,\n",
    "               'R2':r2,\n",
    "               'Mean Squared Error': mse,\n",
    "               'Root Mean Squared Error': rmse}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(machine_learning_delivery_date(df_f1, DecisionTreeRegressor))\n",
    "print(machine_learning_delivery_date(df_f1, ExtraTreesRegressor))\n",
    "print(machine_learning_delivery_date(df_f1, RandomForestRegressor))\n",
    "print(machine_learning_delivery_date(df_f1, AdaBoostRegressor))\n",
    "print(machine_learning_delivery_date(df_f1, GradientBoostingRegressor))\n",
    "print(machine_learning_delivery_date(df_f1, XGBRegressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest algorithm is low bias and reduce overfitting comparedtoothers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using RandomForestRegressor algorithm and predict the 'delivery_date_diff'.\n",
    "# 'item_date_1','delivery date_1' :- these columns are non-numerical and cannot passed, so skip the columns in the modeltraining and prediction\n",
    "\n",
    "def ml_date_difference():\n",
    "\n",
    "    # Train the model by using correct delivery date (df_f1) dataframe\n",
    "    x = df_f1.drop(columns=['item_date_1','delivery date_1','delivery_date_diff'], axis=1)\n",
    "    y = df_f1['delivery_date_diff']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    model = RandomForestRegressor().fit(x_train, y_train)\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    for index, row in df_f2.iterrows():\n",
    "        input_data = row.drop(['item_date_1', 'delivery date_1', 'delivery_date_diff'])\n",
    "        y_pred = model.predict([input_data])\n",
    "        y_pred_list.append(y_pred[0])\n",
    "\n",
    "    return y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning model predict the data difference of (df_f2) dataframe\n",
    "date_difference = ml_date_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert float values into integer using list comprehension method.\n",
    "\n",
    "date_defference1 = [int(round(i,0)) for i in date_difference]\n",
    "print(date_defference1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'delivery_date_diff' column in the datframe\n",
    "df_f2['delivery_date_diff'] = pd.DataFrame(date_defference1)\n",
    "df_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate delivery date (item_date + delivery_date_diff = delivery date)\n",
    "\n",
    "def find_delivery_date(item_date, date_difference):\n",
    "\n",
    "    result_date = item_date + timedelta(days=date_difference)\n",
    "\n",
    "    delivery_date = result_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return delivery_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the delivery date and add to dataframe\n",
    "df_f2['item_date_1'] = pd.to_datetime(df_f2['item_date_1'])\n",
    "df_f2['delivery_date'] = df_f2.apply(lambda x: find_delivery_date(x['item_date_1'], x['delivery_date_diff']), axis=1)\n",
    "df_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally concatenate the both dataframe into single dataframe\n",
    "df_final = pd.concat([df_f1, df_f2], ignore_index=True)\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the day, month, and year from 'delivery date_1' column and add into dataframe (This data also help us to prediction)\n",
    "df_final['delivery date_1'] = pd.to_datetime(df_final['delivery date_1'])\n",
    "df_final['delivery_date_day'] = df_final['delivery date_1'].dt.day\n",
    "df_final['delivery_date_month'] = df_final['delivery date_1'].dt.month\n",
    "df_final['delivery_date_year'] = df_final['delivery date_1'].dt.year  \n",
    "df_final.drop(columns=['item_date','delivery date','delivery_date','item_date_1','delivery date_1','delivery_date_diff'], inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Method - Predict Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df_final.copy()\n",
    "\n",
    "# filter the status column values only 1 & 0 rows in a new dataFrame ['Won':1 & 'Lost':0]\n",
    "df_c = df_c[(df_c.status == 1) | (df_c.status == 0)]\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check no of rows (records) each 1 and 0 in dataframe\n",
    "df_c['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in status feature, the 'Won', and 'Lost' value difference is very high. so need to oversampling to reduce the difference\n",
    "\n",
    "x = df_c.drop('status', axis=1)\n",
    "y = df_c['status']\n",
    "\n",
    "x_new, y_new = SMOTETomek().fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape, x_new.shape, y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of training and testing using metrics\n",
    "# algorithm.__name__ - it return the algorith name\n",
    "def machine_learning_classification(x_new, y_new, algorithm):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "    model = algorithm().fit(x_train, y_train)\n",
    "\n",
    "    y_perd_train = model.predict(x_train)\n",
    "    y_perd_test = model.predict(x_test)\n",
    "\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_perd_train)\n",
    "    accuracy_test = metrics.accuracy_score(y_test, y_perd_test)\n",
    "\n",
    "    # algo = str(algorithm).split(\"'\")[1].split(\".\")[-1]\n",
    "    accuracy_metrics = {'algorithm': algorithm.__name__,\n",
    "                        'accuracy_train': accuracy_train,\n",
    "                        'accuracy_test': accuracy_test}\n",
    "\n",
    "    return accuracy_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(machine_learning_classification(x_new, y_new, DecisionTreeClassifier))\n",
    "print(machine_learning_classification(x_new, y_new, ExtraTreesClassifier))\n",
    "print(machine_learning_classification(x_new, y_new, RandomForestClassifier))\n",
    "print(machine_learning_classification(x_new, y_new, AdaBoostClassifier))\n",
    "print(machine_learning_classification(x_new, y_new, GradientBoostingClassifier))\n",
    "print(machine_learning_classification(x_new, y_new, XGBClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got goos accuracy after oversampling using SMOTETomek method.\n",
    "# ExtraTreeclassifier and randomForestClassifier both have good test accuracy, but in training accuracy is overfitting.\n",
    "# Best performers (generalization): ExtraTreesClassifier (~98.1%), RandomForestClassifier (~97.8%)\n",
    "\n",
    "# Overfitting suspect: DecisionTreeClassifier (train = 100%)\n",
    "\n",
    "# Weaker learners: AdaBoostClassifier and GradientBoostingClassifier\n",
    "\n",
    "# XGBoost: Strong, but not beating ExtraTrees/RandomForest in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridsearchCV is a cross validation function.\n",
    "# hyper parameter tuning - we give parameter values manually in the algorith to reduce the overfitting issue and get better accuracy.\n",
    "\n",
    "# so using gridsearchcv method - to pass the multiple values in each parameters and try to evalute all thecombination of values and\n",
    "# finally return the best accuracy parameter values based on the score.\n",
    "\n",
    "# example: {'max_depth': 20, max_features: 'sqrt', 'min_samples_leaft': 1, 'min_samples_split':2}\n",
    "# Note : this process can take long time (avg: 1 hr 15 mins). Please wait be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer parameter values: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {'max_depth'        : [2, 5, 10, 20],\n",
    "              'min_samples_split':[2, 5, 10],\n",
    "              'min_samples_leaf' :[1, 2, 4],\n",
    "              'max_features'     : ['sqrt', 'log2']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(),param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n -jobs = -1 means it uses the all processorsintheprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalute all the parameter combinations and return the best parameter based on score\n",
    "grid_search.best_params_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the parameters in the random forest algorithm and check the accuracy for training and testing\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the parameters in the random forest algorithm and check the accuracy for training and testing\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=20, max_features='log2', min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the training accuracy overfitting reduced.so now model will predict effectively for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the status and check the accuracy using metrics\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print(classification_report(y_true=y_test,y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find outliers - box plot & skewed data - hist plot and violin plot\n",
    "\n",
    "# def skewness_plot(df, *column):\n",
    "#     number_row = len(column)\n",
    "#     plot_no=0\n",
    "#     for col_name in column:\n",
    "#         if 'log' in col_name or 'sqrt' in col_name or 'boxcox' in col_name:\n",
    "#             title = \"After Transformation\"\n",
    "#         else:\n",
    "#             title = \"Before Transformation\"\n",
    "\n",
    "#         plt.figure(figsize=(18,18))\n",
    "\n",
    "#         plot_no+= 1\n",
    "#         plt.subplot(number_row,3, plot_no)\n",
    "#         sns.boxplot(x=col_name, data=df)\n",
    "#         plt.title('Boxplot - '+ title)\n",
    "\n",
    "#         plot_no += 1\n",
    "#         plt.subplot(number_row, 3, plot_no)\n",
    "#         sns.histplot(df[col_name], bins=30, edgecolor='black')\n",
    "#         plt.title(f'Histogram - Skewness: {df[col_name].skew():.2f}')\n",
    "\n",
    "#         plot_no+=1\n",
    "#         plt.subplot(number_row,3, plot_no)\n",
    "#         sns.violinplot(x=col_name, data=df)\n",
    "#         plt.title('Violinplot -'+ title)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify numerical columns for analysis\n",
    "numerical_columns = ['quantity tons','width','thickness','selling_price']\n",
    "# calling the Skewness_plot function\n",
    "skewness_plot(df, *numerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantity tons, thickness and selling price data are skewd. so using the log transformation method to handle the skewness data\n",
    "\n",
    "def Log_Transformation(df, *column):\n",
    "\n",
    "    for col_name in column:\n",
    "        # Log transformation\n",
    "        df[col_name+'_log'] = np.log1p(df[col_name])\n",
    "\n",
    "    column = [i for i in df.columns if 'log' in i]\n",
    "\n",
    "    return skewness_plot(df, * column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Transformation(df1, *numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['quantity tons','thickness','width','selling_price','quantity tons_log','thickness_log','selling_price_log']\n",
    "sns.heatmap(df1[column_name].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Handling - Inrequartile Range (IQR) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_plot(df):\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='width', data=df)\n",
    "    plt.title('BoxPlot - width')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='quantity tons_log', data=df)\n",
    "    plt.title('BoxPlot - quantity tons_log')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.boxplot(x='thickness_log', data=df)\n",
    "    plt.title('BoxPlot - ' + 'thickness_log')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='selling_price_log', data=df)\n",
    "    plt.title('BoxPlot - ' + 'selling_price_log')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_plot(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using IQR and clip() methods to handel the outliers and add a new column of DataFrame \n",
    "\n",
    "def outlier(df, column):\n",
    "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
    "    upper_threshold = df[column].quantile(0.75) + (1.5*iqr)\n",
    "    lower_threshold = df[column].quantile(0.25) - (1.5*iqr)\n",
    "    df[column] = df[column].clip(lower_threshold, upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Ex: lower threshold = 5 and upeer threshold = 20)\n",
    "# above upeer threshold values (>20) are converted to upeer threshold value (20) in features\n",
    "# below lower threshold values (<5) are converted to lower threshold value (5) in features\n",
    "\n",
    "outlier(df1, 'quantity tons_log')\n",
    "outlier(df1, 'thickness_log')\n",
    "outlier(df1, 'selling_price_log')\n",
    "outlier(df1, 'width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_plot(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_handle_col = ['quantity tons_log', 'width', 'thickness_log', 'selling_price_log']\n",
    "skewness_plot(df1, *outlier_handle_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['quantity tons','thickness', 'width', 'selling_price', 'quantity tons_log', 'thickness_log', 'selling_price_log']\n",
    "sns.heatmap(df1[column_name].corr(), annot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns=['width_log'], inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "\n",
    "# Find the difference between item and delivery date and add the new column in dataframe\n",
    "df2['day_difference'] = (pd.to_datetime(df2['delivery date']) - pd.to_datetime(df2['item_date'])).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['item_date'] = pd.to_datetime(df2['item_date'], format='%y%m%d')\n",
    "df2['delivery date'] = pd.to_datetime(df2['delivery date'], format='%y%m%d')\n",
    "\n",
    "df2['item_date_day'] = df2['item_date'].dt.day\n",
    "df2['item_date_month'] = df2['item_date'].dt.month\n",
    "df2['item_date_year'] = df2['item_date'].dt.year\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the non-negative value of 'Date_difference' column in separate dataFrame\n",
    "\n",
    "non_negative_delivery_date_df = df2[df2['day_difference'] > 0]\n",
    "\n",
    "# split negative value od 'Date_difference' column in another dataframe\n",
    "\n",
    "negative_delivery_date_df = df2[df2['day_difference'] <= 0]\n",
    "\n",
    "non_negative_delivery_date_df.shape, negative_delivery_date_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of that\n",
    "\n",
    "correct_delivery_date = non_negative_delivery_date_df.copy()\n",
    "inconsistance_delivery_date = negative_delivery_date_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (AdaBoostRegressor,\n",
    "                              RandomForestRegressor,\n",
    "                              ExtraTreesRegressor,\n",
    "                              GradientBoostingRegressor,\n",
    "                              HistGradientBoostingRegressor\n",
    "                              )\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.models = {\n",
    "            'AdaboostRegressor': AdaBoostRegressor(),\n",
    "            'RandomForestRegressor': RandomForestRegressor(),\n",
    "            'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "            'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "            'HistGradientBoostingRegressor': HistGradientBoostingRegressor(),\n",
    "            'DecisionTreeRegressor' : DecisionTreeRegressor(),\n",
    "            'XGBRegressor': XGBRegressor()\n",
    "        }\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def evaluate_model(self, model_name, model):\n",
    "        y_train_pred = model.fit(self.x_train, self.y_train).predict(self.x_train)\n",
    "        y_test_pred = model.predict(self.x_test)\n",
    "\n",
    "        result = {\n",
    "            'Model_Name': model_name,\n",
    "            'Train_Mean_Square_Error': f'{mean_squared_error(self.y_train, y_train_pred): .4e}',\n",
    "            'Test_Mean_Square_Error': f'{mean_squared_error(self.y_test, y_test_pred): .4e}',\n",
    "            'Train_Mean_Absolute_Error': round(mean_absolute_error(self.y_train, y_train_pred), 4),\n",
    "            'Test_Mean_Absolute_Error': round(mean_absolute_error(self.y_test, y_test_pred), 4),\n",
    "            'Train_R2_Score': f'{r2_score(self.y_train, y_train_pred): .4e}',\n",
    "            'Test_R2_Score': round(r2_score(self.y_test, y_test_pred), 4),\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def all_models(self):\n",
    "        # Train and evalute each model\n",
    "        results = [self.evaluate_model(model_name, model) for model_name, model in self.models.items()]\n",
    "\n",
    "        self.model_score_df = pd.DataFrame(results)\n",
    "\n",
    "        return self.model_score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical to numerical conversion\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "for col in ['delivery date','item_date', 'status','item type']:\n",
    "    correct_delivery_date[col] = enc.fit_transform(correct_delivery_date[[col]])\n",
    "    inconsistance_delivery_date[col] = enc.fit_transform(inconsistance_delivery_date[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to asign the independend and depentend features in correct_delivery_date\n",
    "\n",
    "y = correct_delivery_date['day_difference']\n",
    "x = correct_delivery_date.drop(['day_difference','item_date', 'delivery date'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models using the 'regression_method\n",
    "\n",
    "models = Regression(x, y)\n",
    "results_df = models.all_models()\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evalute(x, y, inconsistent_data):\n",
    "    # Split the original dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a RandomForestRegressor\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # predictions on training and testing sets\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate mean Square errors\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(f'Train_Mean_Square_Error:{round(mse_train, 4)}')\n",
    "    print(f'Test_Mean_Square_Error: {round(mse_test, 4)}')\n",
    "\n",
    "    # Extract features for inconsistent delivery date data\n",
    "    y_new = inconsistent_data['day_difference']\n",
    "    x_new = inconsistent_data.drop(['day_difference','item_date', 'delivery date'],axis = 1)\n",
    "\n",
    "    # Predict day_difference for inconsistent delivery dates\n",
    "    day_pred = model.predict(x_new)\n",
    "\n",
    "    return day_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_pred = train_and_evalute(x, y, inconsistance_delivery_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'day_difference' in the original Dataframe\n",
    "\n",
    "negative_delivery_date_df['day_difference'] = day_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_delivery_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update \"delivery_date\" with the help of 'day_defference' in the original DataFrame\n",
    "negative_delivery_date_df['delivery date'] = negative_delivery_date_df['item_date'] + pd.to_timedelta(negative_delivery_date_df['day_difference'], unit='d')\n",
    "\n",
    "negative_delivery_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take  a copy for the purpose of saving the data\n",
    "sample = negative_delivery_date_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_delivery_date_df['item_date'] = pd.to_datetime(negative_delivery_date_df['item_date'])\n",
    "\n",
    "# Update the 'delivery date' using apply and a lambda function\n",
    "negative_delivery_date_df['delivery date'] = negative_delivery_date_df.apply(\n",
    "    lambda row: row['item_date'] + pd.Timedelta(days=row['day_difference']), axis=1\n",
    ")\n",
    "negative_delivery_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_copper_data = pd.concat([non_negative_delivery_date_df, negative_delivery_date_df],axis=0, ignore_index=True)\n",
    "\n",
    "# Extract day, mont, and year components from 'delivery date' \n",
    "final_copper_data['delivery date'] = pd.to_datetime(final_copper_data['delivery date'])\n",
    "final_copper_data['delivery_date_day'] = final_copper_data['delivery date'].dt.day\n",
    "final_copper_data['delivery_date_month'] = final_copper_data['delivery date'].dt.month\n",
    "final_copper_data['delivery_date_year'] = final_copper_data['delivery date'].dt.year\n",
    "final_copper_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'item_date' and 'delivery date' columns \n",
    "final_copper_data.drop(['item_date','delivery date'], axis=1, inplace=True)\n",
    "final_copper_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_copper_data.to_csv(\"final_copper_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification method - Predict Status  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\viren\\OneDrive\\Desktop\\IIT-MADARAS(GUVI)\\Industrial Copper Modeling\\Industrial-Copper-Modeling\\final_copper_data.csv\")\n",
    "final_copper_data =pd.DataFrame(df)\n",
    "final_copper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Status' is either 'Won' or 'Lost'\n",
    "final_data = final_copper_data[(final_copper_data['status'] == 'Won') | (final_copper_data['status'] == 'Lost')]\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_data['status'].unique())\n",
    "print(final_data['item type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "columns_to_encode = ['status','item type']\n",
    "\n",
    "for col_name in columns_to_encode:\n",
    "    encoder = OrdinalEncoder()\n",
    "    final_data[col_name] = encoder.fit_transform(final_data[[col_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_data['status'].unique)\n",
    "print(final_data['item type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "class_distribution = final_data['status'].value_counts()\n",
    "\n",
    "# print distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Check if the classes are balanced or imbalanced\n",
    "if len(class_distribution) == 2:\n",
    "    minority_class, majority_class = class_distribution.index\n",
    "    minority_samples = class_distribution[minority_class]\n",
    "    majority_samples = class_distribution[majority_class]\n",
    "\n",
    "    imbalance_ratio = majority_samples / minority_samples\n",
    "    print(\"\\nImbalance Ratio:\",imbalance_ratio)\n",
    "\n",
    "    if imbalance_ratio > 1.5: # Adjust this threshold based on our problem\n",
    "        print(\"The classes are imbalanced.\")\n",
    "    else:\n",
    "        print(\"The classes are balanced.\")\n",
    "else:\n",
    "    print(\"Not a binary classification problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable 'y_new' and feature 'x_new'\n",
    "y_new = final_data['status']\n",
    "x_new = final_data.drop('status',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning_classification(x_new, y_new, algorithm):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = algorithm().fit(x_train, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    return {\n",
    "        'algorithm': algorithm.__name__,\n",
    "        'accuracy_train': accuracy_train,\n",
    "        'accuracy_test' : accuracy_test\n",
    "    }\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    XGBClassifier\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    accuracy_metrics = machine_learning_classification(x_new, y_new, classifier)\n",
    "    results.append(accuracy_metrics)\n",
    "\n",
    "# Creating a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy_train b/w accuracy_test diferance\n",
    "results_df['accuracy_train']-results_df['accuracy_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chossing ExtraTreeClassifier high accuracy on both the training set (1.0) and the test set (0.971)\n",
    "# Fit an ExtraTreeClassifier model to the training set\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ExtraTreesClassifier().fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWAY FROM THIS POINT, THE CODE IS NOT USED IN THE PROJECT\n",
    "# df[\"item_date_1\"] = pd.to_datetime(df[\"item_date\"].astype(str).str.split('.').str[0],format='%y%m%d',errors='coerce')\n",
    "# df[\"delivery_date_1\"] = pd.to_datetime(df[\"delivery date\"].astype(str).str.split('.').str[0],format='%y%m%d',errors='coerce')\n",
    "# df[\"quantity tons\"] = pd.to_numeric(df[\"quantity tons\"],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"item_date_1\"] = pd.to_datetime(df[\"item_date\"], unit=\"s\", errors=\"coerce\")\n",
    "# df[\"delivery date_1\"] = pd.to_datetime(df[\"delivery date\"], unit=\"s\", errors=\"coerce\")\n",
    "# df[\"quantity tons\"] = pd.to_numeric(df[\"delivery date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the \"00000\" datas from \"material_ref\" column\n",
    "\n",
    "df[\"material_ref\"] = df[\"material_ref\"].apply(lambda x: np.nan if str(x).startswith(\"00000\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"material_ref\" have a maximum number of null values ,so we need to drop that column\n",
    "# id is a unique value so we also drop that column\n",
    "\n",
    "df.drop(columns=[\"id\",\"material_ref\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['item_date_1', 'delivery_date_1']].head())\n",
    "print(df[['item_date_1', 'delivery_date_1']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting selling_price values into null values\n",
    "\n",
    "df[\"selling_price\"] = df[\"selling_price\"].apply(lambda x: np.nan if x <= 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanlding the null values using mean(), median() and mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object columns and mode method\n",
    "df[\"item_date_1\"].fillna(df[\"item_date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"delivery date_1\"].fillna(df[\"delivery date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"status\"].fillna(df[\"status\"].mode().iloc[0],inplace=True)\n",
    "df[\"item_date\"].fillna(df[\"item_date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"delivery date\"].fillna(df[\"delivery date\"].mode().iloc[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical columns and median()\n",
    "df[\"quantity tons\"].fillna(df[\"quantity tons\"].median(),inplace=True)\n",
    "df[\"customer\"].fillna(df[\"customer\"].median(),inplace=True)\n",
    "df[\"country\"].fillna(df[\"country\"].median(),inplace=True)\n",
    "df[\"application\"].fillna(df[\"application\"].median(),inplace=True)\n",
    "df[\"thickness\"].fillna(df[\"thickness\"].median(),inplace=True)\n",
    "df[\"selling_price\"].fillna(df[\"selling_price\"].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {'Won':1,\n",
    "          'Draft':2,\n",
    "          'To be approved':3,\n",
    "          'Lost':0,\n",
    "          'Not lost for AM':4,\n",
    "          'Wonderful':5,\n",
    "          'Revised':6,\n",
    "          'Offered':7,\n",
    "          'Offerable':8}\n",
    "\n",
    "df[\"status\"] = df[\"status\"].map(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"item type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_t = {'W':0,\n",
    "          'WI':1,\n",
    "          'S':2,\n",
    "          'Others':3,\n",
    "          'PL':4,\n",
    "          'IPL':5,\n",
    "          'SLAWR':6}\n",
    "\n",
    "df[\"item type\"] = df[\"item type\"].map(item_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"item type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"item_date_1\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delivery date_1\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['item_date_1'].dtype)\n",
    "print(df['delivery date_1'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['item_date_1'].nunique())\n",
    "print(df['delivery date_1'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
